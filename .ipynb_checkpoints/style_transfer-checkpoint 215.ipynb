{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_nWetWWd_ns"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-09-28T05:54:12.705100Z",
     "iopub.status.busy": "2023-09-28T05:54:12.704756Z",
     "iopub.status.idle": "2023-09-28T05:54:12.708334Z",
     "shell.execute_reply": "2023-09-28T05:54:12.707675Z"
    },
    "id": "2pHVBk_seED1"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6msVLevwcRhm"
   },
   "source": [
    "# Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds4o1h4WHz9U"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/style_transfer\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/style_transfer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDyGj8DmXCJI"
   },
   "source": [
    "This tutorial uses deep learning to compose one image in the style of another image (ever wish you could paint like Picasso or Van Gogh?). This is known as *neural style transfer* and the technique is outlined in <a href=\"https://arxiv.org/abs/1508.06576\" class=\"external\">A Neural Algorithm of Artistic Style</a> (Gatys et al.). \n",
    "\n",
    "Note: This tutorial demonstrates the original style-transfer algorithm. It optimizes the image content to a particular style. Modern approaches train a model to generate the stylized image directly (similar to [CycleGAN](./cyclegan.ipynb)). This approach is much faster (up to 1000x).\n",
    "\n",
    "For a simple application of style transfer with a pretrained model from [TensorFlow Hub](https://tfhub.dev), check out the [Fast style transfer for arbitrary styles](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization) tutorial that uses an [arbitrary image stylization model](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2). For an example of style transfer with [TensorFlow Lite](https://www.tensorflow.org/lite), refer to [Artistic style transfer with TensorFlow Lite](https://www.tensorflow.org/lite/examples/style_transfer/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b3XwN9V1nvR"
   },
   "source": [
    "Neural style transfer is an optimization technique used to take two images—a *content* image and a *style reference* image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.\n",
    "\n",
    "This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NyftRTSMuwue"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 16:43:43.126552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Load compressed models from tensorflow_hub\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sc1OLbOWhPCO"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "from os import walk\n",
    "import time\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GM6VEGrGLh62"
   },
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3TLljcwv5qZs"
   },
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yAlRzJZrWM3"
   },
   "source": [
    "Create a simple function to display an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cBX-eNT8PAK_"
   },
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ImagesForTraining\n",
      "./Art/images/images/Vasiliy_Kandinskiy\n"
     ]
    }
   ],
   "source": [
    "## Get lists that store the file names for content and style pictures \n",
    "\n",
    "# folder path\n",
    "dir_path = './ImagesForTraining'\n",
    "\n",
    "# list to store files name\n",
    "allImagesContent = []\n",
    "for (dir_path, dir_names, file_names) in walk(dir_path):\n",
    "    print(dir_path)\n",
    "    allImagesContent.extend(file_names)\n",
    "#print(allImagesContent)\n",
    "\n",
    "\n",
    "dir_path = './Art/images/images/Vasiliy_Kandinskiy'\n",
    "\n",
    "# list to store files name\n",
    "allImagesStyle = []\n",
    "for (dir_path, dir_names, file_names) in walk(dir_path):\n",
    "    print(dir_path)\n",
    "    allImagesStyle.extend(file_names)\n",
    "#print(allImagesStyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(imageA, imageB):\n",
    " # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    " mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    " mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "\t\n",
    " # return the MSE. The lower the error, the more \"similar\" the two images are.\n",
    " return mse_error\n",
    "\n",
    "def compare(imageA, imageB):\n",
    " # Calculate the MSE and SSIM\n",
    " m = mse(imageA, imageB)\n",
    " s = ssim(imageA, imageB)\n",
    "\n",
    " # Return the SSIM. The higher the value, the more \"similar\" the two images are.\n",
    " return s\n",
    "\n",
    "def main(imageA, imageB): \n",
    "\n",
    " # Convert the images to grayscale\n",
    " gray1 = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    " gray2 = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    " # Check for same size and ratio and report accordingly\n",
    " ho, wo, _ = imageA.shape\n",
    " hc, wc, _ = imageB.shape\n",
    " ratio_orig = ho/wo\n",
    " ratio_comp = hc/wc\n",
    " dim = (wc, hc)\n",
    "\n",
    " if round(ratio_orig, 2) != round(ratio_comp, 2):\n",
    "  print(\"\\nImages not of the same dimension. Check input.\")\n",
    "  exit()\n",
    "\n",
    " # Resize first image if the second image is smaller\n",
    " elif ho > hc and wo > wc:\n",
    "  print(\"\\nResizing original image for analysis...\")\n",
    "  gray1 = cv2.resize(gray1, dim)\n",
    "\n",
    " elif ho < hc and wo < wc:\n",
    "  print(\"\\nCompressed image has a larger dimension than the original. Check input.\")\n",
    "  exit()\n",
    "\n",
    " if round(ratio_orig, 2) == round(ratio_comp, 2):\n",
    "  mse_value = mse(gray1, gray2)\n",
    "  ssim_value = compare(gray1, gray2)\n",
    "  print(\"MSE:\", mse_value)\n",
    "  print(\"SSIM:\", ssim_value)\n",
    "  return ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "## Code umschreiben randomisiert\n",
    "## Pictures in richtigem Ordner speichern\n",
    "#Wie sich der Loss entwickelt\n",
    "\n",
    "## Namen von Picture: Content_name + style_name\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "steps_per_epoch = 1\n",
    "\n",
    "open('./StylizedImages/VGG/Metrics', 'w').close()\n",
    "\n",
    "for i in allImagesContent:\n",
    "    counter = 0\n",
    "    \n",
    "    # Change this to + i if you want all of the pictures\n",
    "    \n",
    "    image_size = (1, 244, 244, 3)\n",
    "    \n",
    "    white_noise = np.random.rand(*image_size).astype(np.float32)\n",
    "\n",
    "    # Convert the NumPy array to a TensorFlow tensor\n",
    "    white_noise_tensor = tf.convert_to_tensor(white_noise, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ##Change\n",
    "    test_content = load_img('./ImagesForTraining/' + i)\n",
    "    display.display(tensor_to_image(test_content))\n",
    "    \n",
    "    \n",
    "    for j in allImagesStyle:\n",
    "        \n",
    "        print(\"Starting\")\n",
    "        \n",
    "        test_style = load_img('./Art/images/images/Vasiliy_Kandinskiy/' + j)\n",
    "        \n",
    "        x = tf.keras.applications.vgg19.preprocess_input(white_noise*255)\n",
    "        x = tf.image.resize(x, (224, 224))\n",
    "        vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "        prediction_probabilities = vgg(x)\n",
    "        prediction_probabilities.shape\n",
    "        \n",
    "        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "        \n",
    "        \n",
    "        ## Information from where we are extracting the Information out\n",
    "        content_layers = ['block5_conv2'] \n",
    "\n",
    "        style_layers = ['block1_conv1',\n",
    "                       'block2_conv1',\n",
    "                        'block3_conv1', \n",
    "                       'block4_conv1', \n",
    "                        'block5_conv1']\n",
    "\n",
    "        num_content_layers = len(content_layers)\n",
    "        num_style_layers = len(style_layers)\n",
    "        \n",
    "        \n",
    "        ## Why do we need this?\n",
    "        def vgg_layers(layer_names):\n",
    "          \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n",
    "          # Load our model. Load pretrained VGG, trained on ImageNet data\n",
    "          vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "          vgg.trainable = False\n",
    "\n",
    "          outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "          model = tf.keras.Model([vgg.input], outputs)\n",
    "          return model\n",
    "    \n",
    "        style_extractor = vgg_layers(style_layers)\n",
    "        style_outputs = style_extractor(test_style*255)\n",
    "        \n",
    "        ## Compute the gram Matrix\n",
    "        def gram_matrix(input_tensor):\n",
    "          result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "          input_shape = tf.shape(input_tensor)\n",
    "          num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "          return result/(num_locations)\n",
    "        \n",
    "        ## Compute the style and Content outputs for a give image\n",
    "        class StyleContentModel(tf.keras.models.Model):\n",
    "          def __init__(self, style_layers, content_layers):\n",
    "            super(StyleContentModel, self).__init__()\n",
    "            self.vgg = vgg_layers(style_layers + content_layers)\n",
    "            self.style_layers = style_layers\n",
    "            self.content_layers = content_layers\n",
    "            self.num_style_layers = len(style_layers)\n",
    "            self.vgg.trainable = False\n",
    "\n",
    "          def call(self, inputs):\n",
    "            \"Expects float input in [0,1]\"\n",
    "            inputs = inputs*255.0\n",
    "            preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "            outputs = self.vgg(preprocessed_input)\n",
    "            style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                              outputs[self.num_style_layers:])\n",
    "\n",
    "            style_outputs = [gram_matrix(style_output)\n",
    "                             for style_output in style_outputs]\n",
    "\n",
    "            content_dict = {content_name: value\n",
    "                            for content_name, value\n",
    "                            in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "            style_dict = {style_name: value\n",
    "                          for style_name, value\n",
    "                          in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "            return {'content': content_dict, 'style': style_dict}\n",
    "        \n",
    "        ## Uses the Style Model\n",
    "        extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "\n",
    "        style_targets = extractor(test_style)['style']\n",
    "        content_targets = extractor(white_noise)['content']\n",
    "\n",
    "        results = extractor(tf.constant(white_noise))\n",
    "        \n",
    "        image = tf.Variable(white_noise_tensor)\n",
    "        \n",
    "        def clip_0_1(image):\n",
    "            return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "        \n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "        \n",
    "        style_weight=1e6\n",
    "        content_weight=1e-3\n",
    "        \n",
    "        ## Compute the losses for the style and the content\n",
    "        def style_content_loss(outputs):\n",
    "            style_outputs = outputs['style']\n",
    "            content_outputs = outputs['content']\n",
    "            style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                                   for name in style_outputs.keys()])\n",
    "            style_loss *= style_weight / num_style_layers\n",
    "\n",
    "            print(\"Style loss\")\n",
    "            print(style_loss)\n",
    "\n",
    "            content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                                     for name in content_outputs.keys()])\n",
    "\n",
    "            print(\"Content loss\")\n",
    "            content_loss *= content_weight / num_content_layers\n",
    "            print(content_loss)\n",
    "            loss = style_loss + content_loss\n",
    "            return loss\n",
    "        \n",
    "        total_variation_weight=30\n",
    "        \n",
    "        \n",
    "        ## Make Gradient Descent\n",
    "        @tf.function()\n",
    "        def train_step(image):\n",
    "          with tf.GradientTape() as tape:\n",
    "            outputs = extractor(image)\n",
    "            loss = style_content_loss(outputs)\n",
    "            print(\"Total\")\n",
    "            print(loss)\n",
    "            loss += total_variation_weight*tf.image.total_variation(image)\n",
    "\n",
    "          grad = tape.gradient(loss, image)\n",
    "          opt.apply_gradients([(grad, image)])\n",
    "          image.assign(clip_0_1(image))\n",
    "        \n",
    "        display.display(tensor_to_image(test_style))\n",
    "        \n",
    "        step = 0\n",
    "        for n in range(epochs):\n",
    "          for m in range(steps_per_epoch):\n",
    "            step += 1\n",
    "            train_step(image)\n",
    "            print()\n",
    "            print(\".\", end='', flush=True)\n",
    "          display.clear_output(wait=True)\n",
    "          display.display(tensor_to_image(image))\n",
    "          print(\"Train step: {}\".format(step))\n",
    "          #file_name = 'stylized-image'+str(n)+'.png'\n",
    "          #tensor_to_image(image).save(file_name)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Total time: {:.1f}\".format(end-start))\n",
    "        file_name = i + j + '.png'\n",
    "        tensor_to_image(image).save('./StylizedImages/VGG/' + file_name)\n",
    "        \n",
    "        ## Save Image\n",
    "        #image_pil = tensor_to_image(image)\n",
    "        #image_pil.save(\"Content:\" + i + \"/Style:\" + j)\n",
    "        \n",
    "        \n",
    "        ## Compute Metrics\n",
    "        \n",
    "        \n",
    "        image_style = cv2.imread('./Art/images/images/Vasiliy_Kandinskiy/' + j)\n",
    "        image_result = cv2.imread('./StylizedImages/VGG/' + file_name)\n",
    "        image_content = cv2.imread('./ImagesForTraining/' + i)\n",
    "\n",
    "        display.display(image_result)\n",
    "        height_test, width_test, depth_test = image_style.shape\n",
    "\n",
    "        \n",
    "        image_content = cv2.resize(image_content, (width_test, height_test))\n",
    "\n",
    "        image_result = cv2.resize(image_result, (width_test, height_test))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        channels1 = cv2.split(image_style)\n",
    "        channels2 = cv2.split(image_result)\n",
    "        \n",
    "        hist1 = [cv2.calcHist([channel], [0], None, [256], [0, 256]) for channel in channels1]\n",
    "        hist2 = [cv2.calcHist([channel], [0], None, [256], [0, 256]) for channel in channels2]\n",
    "\n",
    "        ## Normalize for good comparison of Chi Squared\n",
    "        hist1 = [hist / hist.sum() for hist in hist1]\n",
    "        hist2 = [hist / hist.sum() for hist in hist2]\n",
    "        \n",
    "        chi_square_distances = [cv2.compareHist(h1, h2, cv2.HISTCMP_CHISQR) for h1, h2 in zip(hist1, hist2)]\n",
    "\n",
    "        print(chi_square_distances)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ssim_value = main(image_result, image_style)\n",
    "        \n",
    "        with open('./StylizedImages/VGG/Metrics', 'a') as f:\n",
    "            f.write('Image and Style:' + i + j + \"\\n\")\n",
    "        \n",
    "        with open('./StylizedImages/VGG/Metrics', 'a') as f:\n",
    "            f.write('SSIM Value: ' + str(ssim_value) + \"\\n\")\n",
    "            \n",
    "            \n",
    "        with open('./StylizedImages/VGG/Metrics', 'a') as fp:\n",
    "            for item in chi_square_distances:\n",
    "            # write each item on a new line\n",
    "                fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "style_transfer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
